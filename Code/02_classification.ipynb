{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from fiona import crs\n",
    "import rasterio as rio\n",
    "from rasterio import features\n",
    "from rasterio.merge import merge\n",
    "from rasterio.transform import Affine\n",
    "from rasterio.crs import CRS\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from shapely.geometry import Point,Polygon\n",
    "import math\n",
    "from osgeo import gdal\n",
    "# from gdalconst import GA_ReadOnly\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import make_scorer, recall_score,accuracy_score,balanced_accuracy_score, precision_score,f1_score,confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.decomposition import PCA\n",
    "import csv\n",
    "import datetime\n",
    "random.seed(0)\n",
    "np.random.seed(0) \n",
    "%autosave 10\n",
    "%matplotlib inline\n",
    "# -*- coding: utf-8 -*-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define some global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dist_img = r'..\\processing\\green_space_classification\\dist_road\\dist_%s.tif'%ux\n",
    "bands=['ndti','ndre','ndvi','ndwi','mndwi','glcm','B2','B3','B4','B8'] #define the band names in the img\n",
    "\n",
    "# read in all the OSM features that may have vegetation\n",
    "veges = pd.read_excel(r'osm_vegetation_classes.xlsx')\n",
    "\n",
    "foi = ['ndti','ndre','ndvi','ndwi','mndwi','glcm']\n",
    "ind_selected = [bands.index(x) for x in foi]\n",
    "bands = foi\n",
    "\n",
    "accuracy_out = r'..\\Processing\\accuracy_classification.csv'\n",
    "# fail_out = r'D:\\Work\\processing\\green_space_classification\\fail_classification.csv'\n",
    "# print(ind_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = gpd.read_file(r'../Boundaries/Boundaries.shp')\n",
    "cities = cities['ID'].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define some global functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_pnts(row):\n",
    "    # define a function to align samples with raster cell\n",
    "    geometry = row['geometry']\n",
    "    bounds = geometry.bounds\n",
    "    xmin, ymin, xmax, ymax = bounds[0], bounds[1], bounds[2], bounds[3]\n",
    "    x,y= np.mgrid[xmin:xmax+10:10,ymin:ymax+10:10]\n",
    "    x,y = np.vstack([x.ravel(), y.ravel()])\n",
    "    p = pd.DataFrame(list(zip(x,y)))\n",
    "    p[0]=np.floor((p[0]-row['xmin'])/row['xres'])*row['xres']+row['xmin']+row['xres']/2\n",
    "    p[1]=np.floor((p[1]-row['ymin'])/row['yres'])*row['yres']+row['ymin']+row['yres']/2\n",
    "    p['pnt'] = list(set(zip(p[0],p[1])))\n",
    "    p['pnt']  = p['pnt'].apply(Point)\n",
    "    p = gpd.GeoDataFrame(p['pnt'],geometry='pnt',crs=crs.from_epsg(3857))\n",
    "    p = p[p.within(geometry)]\n",
    "    return p['pnt'].apply(lambda x:[x.x,x.y]).values\n",
    "def sample_raster(row,img_array):\n",
    "    # define a function to sample the rasters\n",
    "    y = int(row['y_n'])\n",
    "    x = int(row['x_n'])\n",
    "    res = img_array[:,y,x]\n",
    "    if np.isnan(res).any():\n",
    "        res = np.nan\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the selected feature polygons\n",
    "def extract_OSM_polygons(OSM):\n",
    "    shapefile = gpd.read_file(OSM)\n",
    "    shapefile= shapefile.to_crs({'init': 'epsg:3857'})\n",
    "    shapefile['geometry'] = shapefile.geometry.buffer(-10)\n",
    "    shapefile = shapefile[~shapefile.is_empty]\n",
    "    building = shapefile[~shapefile['building'].isnull()]\n",
    "    building.loc[:,'area_length']=(building.area/building.length).values\n",
    "    building.loc[:,'general'] = 'bldg'\n",
    "    shapefile = shapefile[(shapefile['boundary'].isnull())&shapefile['building'].isnull()]\n",
    "    shapefile['FID'] = list(range(0,len(shapefile.index)))\n",
    "    one_city = pd.DataFrame()\n",
    "    for i in veges.index:\n",
    "        sub = pd.DataFrame()\n",
    "        key = veges.loc[i,'Key']\n",
    "        value = veges.loc[i,'Value']\n",
    "        sub['geometry'] = shapefile.loc[shapefile[key]==value,'geometry']\n",
    "        sub['FID'] = shapefile.loc[shapefile[key]==value,'FID']\n",
    "        sub['key'] = key\n",
    "        sub['value']=value\n",
    "        sub['SALID1'] = OSM.split('\\\\')[-1].split('.')[0]\n",
    "        if len(sub.index)>0:\n",
    "            one_city=pd.concat([one_city,sub])\n",
    "    one_city['general']='vegetation'\n",
    "\n",
    "    if len(one_city.index)>0:\n",
    "        one_city_gdf = gpd.GeoDataFrame(one_city,geometry='geometry', crs=crs.from_epsg(3857))\n",
    "        one_city_gdf.loc[:,'shape_index']=(one_city_gdf.length/(4*np.sqrt(one_city_gdf.area))).values\n",
    "        one_city_gdf = one_city_gdf.loc[(one_city_gdf.area<=one_city_gdf.area.quantile(0.975))&\n",
    "              (one_city_gdf.area>=one_city_gdf.area.quantile(0.025))&\n",
    "              (one_city_gdf['shape_index']<one_city_gdf['shape_index'].quantile(0.9))]\n",
    "\n",
    "        background=shapefile.loc[~shapefile['FID'].isin(set(one_city['FID'])),['geometry','FID']]\n",
    "        background['general']='other'\n",
    "        background.loc[:,'shape_index']=(background.length/(4*np.sqrt(background.area))).values\n",
    "        background = background.loc[(background.area<=background.area.quantile(0.975))&\n",
    "          (background.area>=background.area.quantile(0.025))&\n",
    "          (background['shape_index']<background['shape_index'].quantile(0.9))]\n",
    "        one_city_gdf = pd.concat([one_city_gdf,building])\n",
    "        one_city_gdf = pd.concat([one_city_gdf,background])\n",
    "        one_city_gdf = gpd.GeoDataFrame(one_city_gdf[['general','geometry','shape_index']],geometry='geometry', crs=crs.from_epsg(3857))\n",
    "    one_city_gdf.to_file(driver = 'ESRI Shapefile', filename= r\"..\\Processing\\polygon_%s.shp\"%city)\n",
    "    return one_city_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample(one_city_gdf):\n",
    "    global img,city,bands\n",
    "    # sample points to raster grid\n",
    "    raster = gdal.Open(img, gdal.GA_ReadOnly)\n",
    "    geoTransform = raster.GetGeoTransform()\n",
    "    one_city_gdf['xmin'] = geoTransform[0]\n",
    "    one_city_gdf['ymin'] = geoTransform[3]\n",
    "    one_city_gdf['xres'] = geoTransform[1]\n",
    "    one_city_gdf['yres'] = geoTransform[5]\n",
    "    one_city_gdf['pnts']=  one_city_gdf.apply(sample_pnts,axis=1)\n",
    "    # attach sample class\n",
    "    all_sample = gpd.GeoDataFrame()\n",
    "    for i in set(one_city_gdf['general']):\n",
    "        xys = one_city_gdf.loc[one_city_gdf['general']==i,'pnts'].values\n",
    "        xys_flat = [item for sublist in xys for item in sublist]\n",
    "        sample_df = pd.DataFrame(xys_flat)\n",
    "        sample_df['coordinates'] = list(zip(sample_df[0],sample_df[1]))\n",
    "        sample_gdf = gpd.GeoDataFrame(sample_df['coordinates'],\n",
    "                                      geometry=gpd.points_from_xy(sample_df[0],sample_df[1]),crs=\"epsg:3857\")\n",
    "        sample_gdf['class'] = i\n",
    "        all_sample = pd.concat([all_sample,sample_gdf])\n",
    " \n",
    "    # attach sample to img grid x,y\n",
    "    all_sample['x']=all_sample.geometry.x\n",
    "    all_sample['y']=all_sample.geometry.y\n",
    "    all_sample['x_n'] = (all_sample['x'] - geoTransform[0])/geoTransform[1]-0.5\n",
    "    all_sample['y_n'] = (all_sample['y'] - geoTransform[3])/geoTransform[5]-0.5\n",
    "    all_sample = all_sample.reset_index()\n",
    "\n",
    "    # remove overlapped samples\n",
    "    land_sample = all_sample[all_sample['class']!='bldg'].copy()\n",
    "    land_sample.drop_duplicates(subset=['x','y'], keep=False, inplace=True)\n",
    "    clean_sample = pd.concat([land_sample,all_sample[all_sample['class']=='bldg'].copy()])\n",
    "    clean_sample['bldg_drop']=0\n",
    "    clean_sample.loc[clean_sample['class']=='bldg','bldg_drop']=1\n",
    "    clean_sample = clean_sample.sort_values('bldg_drop', ascending=True)\n",
    "    clean_sample.drop_duplicates(subset=['x','y'], keep='last', inplace=True)\n",
    "    \n",
    "    # mask out pixels with nan in any band\n",
    "    img_array= np.array(raster.ReadAsArray())\n",
    "    print(img_array.shape)\n",
    "    img_array = img_array[ind_selected,:,:]\n",
    "    x_d = img_array.shape[1]\n",
    "    y_d = img_array.shape[2]\n",
    "    n_d =  img_array.shape[0]\n",
    "    img_array = img_array.reshape(n_d,x_d*y_d)\n",
    "    img_array[:,np.isnan(img_array).any(axis=0)] = np.nan\n",
    "    img_array = img_array.reshape(n_d,x_d,y_d)\n",
    "    # use the samples to sample the img\n",
    "    \n",
    "    clean_sample['sample_value'] = clean_sample.apply(lambda x:sample_raster(x,img_array),axis=1)\n",
    "    #remove bad sample based on ndvi\n",
    "    clean_sample = clean_sample.loc[~clean_sample['sample_value'].isnull()]\n",
    "    clean_sample['mean_ndvi'] = clean_sample['sample_value'].apply(lambda x:x[bands.index('ndvi')])\n",
    "    # drop any vegetation sample with NDVI less than 0.1\n",
    "    clean_sample.loc[(clean_sample['class']=='vegetation')\n",
    "                     &(clean_sample['mean_ndvi']<=0.1),'sample_value']=np.nan \n",
    "    clean_sample = clean_sample.loc[~clean_sample['sample_value'].isnull()]\n",
    "\n",
    "    # drop any non-vegetation sample with NDVI greater than median NDVI of vegetated samples\n",
    "    v_median = clean_sample.loc[(clean_sample['class']=='vegetation'),'mean_ndvi'].median()\n",
    "    clean_sample.loc[(clean_sample['class']!='vegetation')\n",
    "                     &(clean_sample['mean_ndvi']>=v_median),'sample_value']=np.nan\n",
    "    clean_sample = clean_sample.loc[~clean_sample['sample_value'].isnull()]\n",
    "    clean_sample = clean_sample.drop(columns=['mean_ndvi','sample_value'],axis=1)\n",
    "    # PCA transformation of the img\n",
    "    # min-max normalization first\n",
    "    for i in range(0,img_array.shape[0]):\n",
    "        v = img_array[i,:,:]\n",
    "        img_array[i,:,:]=(v-np.nanmin(v))/(np.nanmax(v)-np.nanmin(v))\n",
    "    img_array_pca = np.copy(img_array)\n",
    "    img_array_pca = img_array_pca.reshape((img_array_pca.shape[0],\n",
    "                                           img_array_pca.shape[1]*img_array_pca.shape[2])).transpose()\n",
    "    img_array_pca_valid = img_array_pca[~np.isnan(img_array_pca).any(axis=1)]\n",
    "    pca = PCA(n_components=img_array_pca_valid.shape[1])\n",
    "    pca_res = pca.fit(img_array_pca_valid)\n",
    "    var=np.cumsum(np.round(pca_res.explained_variance_ratio_, decimals=3)*100)\n",
    "    n_pc = sum(var<=90)+1\n",
    "    pca = PCA(n_components=n_pc)\n",
    "    pca_reduce = pca.fit_transform(img_array_pca_valid)\n",
    "    pca_reduce = np.multiply(pca_reduce,pca_res.explained_variance_ratio_[:n_pc])\n",
    "\n",
    "    img_reduce = np.copy(img_array[:n_pc,:,:])\n",
    "    img_reduce_re = img_reduce.reshape((img_reduce.shape[0],img_reduce.shape[1]*img_reduce.shape[2])).transpose()\n",
    "    img_reduce_re[~np.isnan(img_reduce_re).any(axis=1)] = pca_reduce\n",
    "    img_reduce_re = img_reduce_re.transpose()\n",
    "    img_reduce = img_reduce_re.reshape((img_reduce.shape[0],img_reduce.shape[1],img_reduce.shape[2]))\n",
    "\n",
    "    img_array = np.copy(img_reduce)\n",
    "    del img_array_pca,img_array_pca_valid,img_reduce_re,img_reduce,pca_reduce\n",
    "    # determine outliers in the samples\n",
    "    clean_sample['sample_value'] = clean_sample.apply(lambda x: sample_raster(x,img_array),axis=1)\n",
    "    PCAs = list(range(0,n_pc))\n",
    "    for PC in PCAs:\n",
    "        i  = PC\n",
    "        clean_sample[PC] = clean_sample['sample_value'].apply(lambda x:x[i])\n",
    "    clean_sample= clean_sample.drop('sample_value',axis=1)\n",
    "\n",
    "    for i in set(clean_sample['class']):\n",
    "        X = clean_sample.loc[clean_sample['class']==i,PCAs].values\n",
    "        X = np.array(X.tolist())\n",
    "        clf = LocalOutlierFactor(n_neighbors=20, contamination='auto')\n",
    "        y_pred = clf.fit_predict(X)\n",
    "        clean_sample.loc[clean_sample['class']==i,'outlier']=y_pred\n",
    "        outlier_score = clf.negative_outlier_factor_\n",
    "        clean_sample.loc[clean_sample['class']==i,'outlier_score'] = (outlier_score-outlier_score.min()) / (outlier_score.max() - outlier_score.min())\n",
    "    clean_sample[PCAs] = clean_sample[PCAs].astype(np.float32)\n",
    "    clean_sample = clean_sample.dropna()\n",
    "    clean_sample = clean_sample.loc[clean_sample['outlier']!=-1]\n",
    "\n",
    "    # random selection of samples\n",
    "    n_vege = len(clean_sample.loc[clean_sample['class']=='vegetation'])\n",
    "    n_other = len(clean_sample.loc[clean_sample['class']=='other'])\n",
    "    n_bldg = len(clean_sample.loc[clean_sample['class']=='bldg'])\n",
    "    n_sample = int(0.2*min(n_vege,n_other+n_bldg))\n",
    "    if n_sample>=2500:\n",
    "        n_sample = 2500\n",
    "    if n_sample<200:\n",
    "        n_sample = int(1*min(n_vege,n_other+n_bldg))\n",
    "    sub_clean_sample = clean_sample.loc[clean_sample['class']=='vegetation'].sample(n=n_sample,random_state=0)\n",
    "    if n_other>(n_sample/2) and n_bldg>(n_sample/2):\n",
    "        sub_clean_sample = pd.concat([sub_clean_sample,clean_sample.loc[clean_sample['class']=='bldg'].sample(n=int(n_sample/2),random_state=0)])\n",
    "        sub_clean_sample = pd.concat([sub_clean_sample,clean_sample.loc[clean_sample['class']=='other'].sample(n=int(n_sample/2),random_state=0)])\n",
    "    else:\n",
    "        if n_other>(n_sample/2) and n_bldg<(n_sample/2):\n",
    "\n",
    "            sub_clean_sample = sub_clean_sample.append(clean_sample.loc[clean_sample['class']=='bldg'].sample(n=int(n_bldg),random_state=0))\n",
    "\n",
    "            sub_clean_sample = sub_clean_sample.append(clean_sample.loc[clean_sample['class']=='other'].sample(n=int(n_sample-n_bldg),random_state=0))\n",
    "\n",
    "        else:\n",
    "\n",
    "            sub_clean_sample = sub_clean_sample.append([clean_sample.loc[clean_sample['class']=='bldg'].sample(n=int(n_sample-n_other),random_state=0)])\n",
    "            sub_clean_sample = sub_clean_sample.append([clean_sample.loc[clean_sample['class']=='other'].sample(n=int(n_other),random_state=0)])\n",
    "    sub_clean_sample.loc[sub_clean_sample['class']=='bldg','class']='other'\n",
    "    ext=sub_clean_sample[['class','geometry']]\n",
    "    ext = ext.to_crs({'init': 'epsg:3857'})\n",
    "    ext.to_file(r\"..\\Processing\\all_sample%s.shp\"%city)\n",
    "    return sub_clean_sample, img_array,PCAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sample(sub_clean_sample,PCAs):\n",
    "    global city\n",
    "    # training testing split\n",
    "    ft = sub_clean_sample[PCAs]\n",
    "    ft['outlier_score'] = sub_clean_sample['outlier_score']\n",
    "\n",
    "    sub_clean_sample['class'] = sub_clean_sample['class'].apply(lambda x:1 if x=='vegetation' else 0)\n",
    "    targets = sub_clean_sample['class']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(ft, targets, stratify=targets,random_state=0)\n",
    "    X_test_weight = X_test['outlier_score']\n",
    "    X_test = X_test[PCAs]\n",
    "    X_train_weight = X_train['outlier_score']\n",
    "    X_train = X_train[PCAs]\n",
    "    train_exp = X_train.merge(sub_clean_sample,how='inner')\n",
    "    test_exp = X_test.merge(sub_clean_sample,how='inner')\n",
    "    gpd.GeoDataFrame(train_exp[['class','geometry']],geometry='geometry').to_file(driver = 'ESRI Shapefile',\n",
    "                                filename= r\"..\\Processing\\train_sample%s.shp\"%city)\n",
    "    gpd.GeoDataFrame(test_exp[['class','geometry']],geometry='geometry').to_file(driver = 'ESRI Shapefile',\n",
    "                                filename= r\"..\\Processing\\test_sample%s.shp\"%city)\n",
    "    return X_train,X_test,y_train,y_test,X_train_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_wrapper(refit_score,clf,param_grid,scorers,X_train,X_test,y_train,y_test,fit_params):\n",
    "    global city\n",
    "    \"\"\"\n",
    "    fits a GridSearchCV classifier using refit_score for optimization\n",
    "    prints classifier performance metrics\n",
    "    \"\"\"\n",
    "    skf = StratifiedKFold(n_splits=10,random_state=0,shuffle=True)\n",
    "    grid_search = GridSearchCV(clf, param_grid, scoring= ['f1','f1_weighted'], refit=refit_score,\n",
    "                           cv=skf, return_train_score=False,n_jobs=4,verbose=2)\n",
    "    grid_search.fit(X_train.values, y_train.values)\n",
    "\n",
    "    # make the predictions\n",
    "    y_pred = grid_search.predict(X_test.values)\n",
    "\n",
    "    return grid_search, {'city':city,'datetime':datetime.datetime.now(),\n",
    "                         'accuracy_balanced':balanced_accuracy_score(y_test,y_pred),\n",
    "                        'accuracy':accuracy_score(y_test,y_pred),\n",
    "                        'precision':precision_score(y_test,y_pred),\n",
    "                         'recall':recall_score(y_test,y_pred),\n",
    "                         'f1_score':f1_score(y_test,y_pred)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for city in ['CO_001']: # use this line for toy example\n",
    "# for city in cities: # use this line for full example\n",
    "    print(city)\n",
    "    OSM = r'..\\OSM\\%s.shp'%city\n",
    "    img = r'..\\Sample_image\\%s.tif'%city # read in the img for classification\n",
    "    one_city_gdf = extract_OSM_polygons(OSM) #get OSM polygons\n",
    "    sub_clean_sample,img_array,PCAs = generate_sample(one_city_gdf) # generate random samples\n",
    "    X_train,X_test,y_train,y_test,X_train_weight = split_sample(sub_clean_sample,PCAs) # sample values, train test split\n",
    "    scorers = {\n",
    "        'precision_score': make_scorer(precision_score),\n",
    "        'recall_score': make_scorer(recall_score),\n",
    "        'accuracy_score': make_scorer(accuracy_score),\n",
    "        'f1_score': make_scorer(f1_score)\n",
    "    }\n",
    "    fit_params = {'sample_weight':X_train_weight}\n",
    "    clf = SVC()\n",
    "    param_grid = {'C':[ 2**x for x in np.arange(-3,13,dtype=float)],\n",
    "                 'gamma':[ 2**x for x in np.arange(-3,13,dtype=float)],\n",
    "                  'random_state':[0],\n",
    "                 'class_weight':['balanced']}\n",
    "    grid_search_clf,test_scores = grid_search_wrapper('f1_weighted',clf,param_grid,scorers,X_train,X_test,y_train,y_test,fit_params)\n",
    "    # save testing accuracy\n",
    "    print(test_scores)\n",
    "    with open(accuracy_out, 'a') as csv_file:\n",
    "        writer = csv.writer(csv_file,delimiter=',',lineterminator='\\n')\n",
    "        writer.writerow(zip(test_scores.keys(),test_scores.values()))\n",
    "    csv_file.close()\n",
    "    # save original img\n",
    "    img_array2 = np.copy(img_array)\n",
    "    img_re = img_array2.reshape((img_array2.shape[0],img_array2.shape[1]*img_array2.shape[2])).transpose()\n",
    "    img_pre = np.copy(img_re[~np.isnan(img_re).any(axis=1)])\n",
    "    img_pre = grid_search_clf.predict(img_pre)\n",
    "    img_pre  = img_pre.astype(np.int16)\n",
    "    res = img_re[:,0]\n",
    "    res[~np.isnan(res)] = img_pre\n",
    "    res[np.isnan(res)] = -32768\n",
    "    res = res.reshape(img_array[0,:,:].shape)\n",
    "    res = res.astype(np.int16)\n",
    "\n",
    "    org_img = gdal.Open(img, gdal.GA_ReadOnly)\n",
    "    meta = {'driver': 'GTiff', 'dtype': 'float32', 'nodata': None, 'width': 1312, 'height': 1135, 'count': 18, 'crs': CRS.from_dict(init='epsg:3857'), 'transform': Affine(10.0, 0.0, -6552380.0,\n",
    "           0.0, -10.0, -3178540.0), 'blockxsize': 256, 'blockysize': 256, 'tiled': True, 'compress': 'lzw', 'interleave': 'pixel'}\n",
    "    meta['width'] = res.shape[1]\n",
    "    meta['height'] = res.shape[0]\n",
    "    meta['count'] = 1\n",
    "    meta['dtype'] = 'int16'\n",
    "    meta['transform'] = Affine(10,0.0,org_img.GetGeoTransform()[0],0,-10,org_img.GetGeoTransform()[-3])\n",
    "    meta['nodata']=-32768\n",
    "    with rio.open(r'..\\Results\\%s.tif'%city, 'w', **meta) as dst:\n",
    "        dst.write(res, 1)\n",
    "    del meta\n",
    "    del org_img\n",
    "    dst.close()\n",
    "    with rio.open(r'..\\Results\\%s.tif'%city) as src:\n",
    "        meta = src.profile\n",
    "        meta['nodata']=-32768\n",
    "        meta['count']=1\n",
    "        meta['dtype'] = 'int16'\n",
    "    src.close()\n",
    "    print('Job done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
